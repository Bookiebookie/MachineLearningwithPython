{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Round 1 - Components of Machine Learning\n",
    "\n",
    "<img src=\"https://github.com/Bookiebookie/MachineLearningwithPython/blob/master/R1_ComponentsML/AMLProblem.png?raw=true\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "Many machine learning (ML) problems and methods consist of three components: \n",
    "\n",
    "1. <b>Data</b> points as the basic (atomic) unit of information. Data points are characterized by features, which are  properties that can be measured (or computed) easily. Besides features, data points are often associated with certain labels that represent some higher-level information or quantity of interest. In contrast to features, labels are difficult to acquire and much of machine learning is about to develop methods that allow to estimate or predict the labels of a data point based on its features.  \n",
    "\n",
    "2. A <b>hypothesis</b> space (also referred to as a ML model) consisting of computationally feasible predictor functions.\n",
    "\n",
    "3. A <b>loss function</b> that is used to assess the quality of a particular predictor function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Goals\n",
    "\n",
    "* Learn to make useful definitions for what data points (examples, samples), features and labels are in different real-life applications. \n",
    "* Learn how to represent data as numpy arrays which are, in turn, the Python implemenation of vectors and matrices.   \n",
    "* Learn to use (\"toy\") datasets provided by the Python library `scikit-learn`. \n",
    "* Learn about the concept of hypothesis spaces. \n",
    "* Learn how to fit (linear) predictions functions to data. \n",
    "\n",
    "This notebook contains several student tasks which require you to write a few lines of Python code to solve small problems. In particular, you have to fill in the gaps marked as **Student Task**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><center><font size=4>Additional material</font></center></b>\n",
    "\n",
    "<b><font size=4>Videos</font></b>\n",
    "\n",
    "* [Data](https://youtu.be/WWYRH3x7_5M), [Hypothesis Space](https://youtu.be/CDcRfak1Mh4), [Hypothesis Space of Linear Models](https://youtu.be/Mch5hmhVuiA), [Hypothesis Space of Decision Trees](https://youtu.be/0FmaLfjAaRE), [Hypothesis Space of Deep Learning](https://youtu.be/im8mweIrpAM),[Loss Functions](https://www.youtube.com/watch?v=Uv9lihDfsBs&t=4s)\n",
    "\n",
    "<b><font size=4>Tutorials</font></b>\n",
    "\n",
    "* components of ML can be found under [this link](https://arxiv.org/pdf/1910.12387.pdf) \n",
    "\n",
    "* Python library `numpy` can be found under [this link](https://hackernoon.com/introduction-to-numpy-1-an-absolute-beginners-guide-to-machine-learning-and-data-science-5d87f13f0d51).\n",
    "\n",
    "* \"Learn the Basics\" and \"Data Science Tutorial\" sections from [this link](https://www.learnpython.org/en/).\n",
    "\n",
    "* a quick refresher for basic properties of matrices can be found under [this link](http://math.mit.edu/~gs/linearalgebra/linearalgebra5_1-3.pdf)\n",
    "and [this link](https://onlinelibrary.wiley.com/doi/pdf/10.1002/9780470549094.app1)\n",
    "\n",
    "* mathematical notation [this link](https://en.wikipedia.org/wiki/List_of_mathematical_symbols)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data as Matrices and Vectors\n",
    "<a id=\"Q1\"></a>\n",
    "\n",
    "To implement ML methods, we need to be able to efficiently **store and manipulate** data.  A quite powerful tool to represent and manipulate data are [vectors and matrices](https://en.wikipedia.org/wiki/Matrix_(mathematics)) which are, in turn, special cases of [tensors](https://en.wikipedia.org/wiki/Tensor). \n",
    "\n",
    "The data points arising in many application domains can often be characterized by a list of numeric attributes. This numeric attributes or \"features\", $x_{r}$ can be stacked conveniently into a vector $\\mathbf{x}=\\big(x_{1},\\ldots,x_{n}\\big)^{T}$. Many ML methods, such as linear regression (see round 2) or logistic regression (see round 3), use predictor functions of the form $h(\\mathbf{x}) = \\mathbf{w}^{T} \\mathbf{x}$ with some weight vector $\\mathbf{w}$. \n",
    "\n",
    "Once we restrict ourselves to linear functions of the form $h(\\mathbf{x}) = \\mathbf{w}^{T} \\mathbf{x}$, we can represent a predictor function by the weight vector $\\mathbf{w}$. Indeed, given the weight vector $\\mathbf{w}$, we can evaluate the predictor function for any feature vector $\\mathbf{x}$ as $h(\\mathbf{x}) = \\mathbf{w}^{T} \\mathbf{x}$. Thus, not noly we can represent data using a vector, but also the predictor functions applied to this data. \n",
    "\n",
    "Assume we have a set of data points which we index with $i=1,...,m$. The $i$th data point is characterized by the feature vector $\\mathbf{x}^{(i)} = \\big( x_{1}^{(i)}, \\ldots, x^{(i)}_{n} \\big)^{T}$ \n",
    "Accepted way to organize the data in ML is following: features are stored in the (\"feature\") matrix **X** with each row containing the data for each data point ($m$ - number of data points) and with each column storing the data of each feature vector ($n$ - number of features):\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{X}  = \\begin{pmatrix} X_{1,1} & X_{1,2}& \\ldots & X_{1,n} \\\\ \n",
    "X_{2,1} & X_{2,2}& \\ldots & X_{2,n} \\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\vdots \\\\ \n",
    "X_{m,1} & X_{m,2} & \\ldots & X_{m,n} \\end{pmatrix}\\in \\mathbb{R}^{m \\times n}   \\quad \\quad (Eq.1)\n",
    "\\end{equation} \n",
    "\\\n",
    "The matrix $\\mathbf{X} \\in \\mathbb{R}^{m \\times n}$ is stored in Python as a numpy array of shape (m,n). The $i$th row of the matrix $\\mathbf{X}$ is the feature vector $\\mathbf{x}^{(i)}$ of the $i$th data point. \n",
    "\n",
    "Labels of data points are stored in vector **y**: \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{y}  = \\begin{pmatrix} y_{1} \\\\ y_{2} \\\\ \\vdots \\\\ y_{m} \\end{pmatrix}\\in \\mathbb{R}^{m}\n",
    "\\end{equation} \n",
    "\\\n",
    "**y** vector is represented as a numpy array of shape (m,1)\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "$m$ - number of data points\\\n",
    "$n$ - number of features\\\n",
    "$\\mathbf{X}$       - upper-case bold letters denote a matrix  \\\n",
    "$\\mathbf{x}$       - lower-case bold letters denote a vector  \\\n",
    "$\\mathbf{x}^{T}$   - transpose of vector x \\\n",
    "$x_{1}$            - first entry of vector x\\\n",
    "$x_{r}$            - $r$th entry of vector x\\\n",
    "$\\mathbf{x}^{(i)}$ - feature vector of $i$th data point\\\n",
    "$x_{r}^{(i)}$      - $r$th feature of $i$th data point\\\n",
    "$\\mathbb{R}$       - real numbers\\\n",
    "$\\mathbb{R}^{n}$   - [real coordinate space](https://en.wikipedia.org/wiki/Real_coordinate_space) consisting of length-$n$ lists of real numbers \\\n",
    "$\\mathbb{R}^{m \\times n}$ - matrices with $m$ rows and $n$ columns of real-valued numbers$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features and Labels "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us illustrate the main ML terminology using a concrete example. Imagine that we want to build a model for classifying songs according to their genre (such as \"Pop\", \"Blues\" or \"Hip-Hop\"). In this case the **data points** will be songs, one particular song correspond to one particular data point. To build a classifier for the song genre, we need some labeled data points, i.e., songs for which we know the correct genre. Each data point has several   **features**, which characterize the songs. Features include e.g., the city where the song was produced, the length of the song's lyrics, its tempo or even the power spectrum of audio signal. The quantity of interest or **label** in this case is the genre to which the song belongs to. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/Bookiebookie/MachineLearningwithPython/blob/master/R1_ComponentsML/FeaturesLabels.jpg?raw=true\" alt=\"Drawing\" style=\"width: 1000px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Bonus1'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <b>Bonus Task.</b> Machine Learning in your life. \n",
    "    \n",
    "Bonus task worth of 50 points.\n",
    "    \n",
    "Produces a short video/slides/description where some real-life situation is modelled as a machine learning problem. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Bonus Task Answer:</b> Weather prediction could be modelled as a machine learning problem.\n",
    "In this model, the data points is the sensor data in one data in a specifict location or area. The feautures in terms of data include pressure, humidity, temperature, wind, etc. And the lables include raining, sunny, cloudy etc. Although most of the features used in this model are digits, the labels are not. So this should be a classification problem. We collect data in different places for a rather long time to build this model and then use loss function to verify how applicable this model is. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-Learn Data\n",
    "\n",
    "The Python library `scikit-learn` comes with a few standard datasets, for instance the [iris](https://scikit-learn.org/stable/datasets/index.html#iris-plants-dataset) and [digits](https://scikit-learn.org/stable/datasets/index.html#optical-recognition-of-handwritten-digits-dataset) datasets for classification and the [boston house prices](https://scikit-learn.org/stable/datasets/index.html#boston-house-prices-dataset) and [linnerrud](https://scikit-learn.org/stable/datasets/index.html#linnerrud-dataset) datasets for regression.\n",
    "These are [Toy datasets](https://scikit-learn.org/stable/datasets/index.html#toy-datasets) - small datasets that do not require to download any file from some external websites. However, `sciki-learn` also provides significantly larger datasets that are referred to as [Real world datasets](https://scikit-learn.org/stable/datasets/index.html#real-world-datasets) which can be accessed online. \n",
    "\n",
    "Find more information about `scikit-learn` datasets here: https://scikit-learn.org/stable/datasets/index.html\n",
    "\n",
    "More datasets can be found here:\n",
    "https://archive.ics.uci.edu/ml/index.php\n",
    "https://www.kaggle.com/datasets\n",
    "\n",
    "Let us now take a closer look on some of these `scikit-learn` datasets and try to identify features and labels for these datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code snippet below shows how to download datasets from `sklearn` and how to access features and labels of the data points in these datasets. Small toy datasets are imported using command `from sklearn import datasets`. \n",
    "\n",
    "These datasets are stored using the [`bunch` data type](https://pypi.org/project/bunch/), which is similar to the `dictionary` data type. A `bunch` object containes key-value pairs. Most datasets contain at least the keys `'data', 'target', 'target_names','DESCR'`. The value of the key `DESCR` is a short description of the dataset. The value of the `'target_names'` and `'target'` keys are the labels' names and labels, respectively, for each data point. \n",
    "By default, the labels of data points are always numbers. \n",
    "\n",
    "In a classification problem, these numbers are integers starting from $0$. The values of the key ``target_names`` provide a textual description of the meaning of different label values. E.g., the labels of images could be $y=0$ or $y=1$ and the label names would be 0=\"Cat\", 1=\"Dog\". \n",
    "The value of the `'data'`key is the feature matrix (see <a href='#Q1'>(Eq.1)</a>). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><center><font size=3>Explore the dataset</font></center></b>\n",
    "The \n",
    "**[\"Digits\" dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits)** contains images of hand-written digits. This dataset can be used for testing a classification method to [recognize digits from hand-written images](https://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html#recognizing-hand-written-digits).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-2f33643f52fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# import toy datasets from sklearn library\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# load the digits dataset into the bunch object \"digits\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdigits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_digits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_distributor_init\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__check_build\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_versions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshow_versions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_IS_32BIT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m _DEFAULT_TAGS = {\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataConversionWarning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdeprecation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeprecated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfixes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnp_version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m from .validation import (as_float_array,\n\u001b[1;32m     29\u001b[0m                          \u001b[0massert_all_finite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/fixes.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlsqr\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msparse_lsqr\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/scipy/stats/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdivision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmorestats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/scipy/stats/stats.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mspecial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmstats_basic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m from ._stats_mstats_common import (_find_repeats, linregress, theilslopes,\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/scipy/stats/distributions.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdivision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m from ._distn_infrastructure import (entropy, rv_discrete, rv_continuous,\n\u001b[0m\u001b[1;32m     11\u001b[0m                                     rv_frozen)\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# for functions of continuous distributions (e.g. moments, entropy, cdf)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mintegrate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# to approximate the pdf of a continuous distribution given its cdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/scipy/integrate/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mquadrature\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0modepack\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mquadpack\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_ode\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_bvp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msolve_bvp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/scipy/integrate/quadpack.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfunctools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_quadpack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import toy datasets from sklearn library\n",
    "from sklearn import datasets \n",
    "\n",
    "# load the digits dataset into the bunch object \"digits\"\n",
    "digits = datasets.load_digits() \n",
    "# print the keys of all (key,value) pairs contained in digits\n",
    "digits.keys() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
