{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "34de0efc9ddba3293e448b0f08b02db3",
     "grade": false,
     "grade_id": "Introduction",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Round 1 - Components of Machine Learning\n",
    "\n",
    "<img src=\"../../../coursedata/R1_ComponentsML/AMLProblem.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "Many machine learning (ML) problems and methods consist of three components: \n",
    "\n",
    "1. Data points as the basic (atomic) unit of information. Data points are characterized by features, which are  properties that can be measured (or computed) easily. Besides features, data points are often associated with certain labels that represent some higher-level information or quantity of interest. In contrast to features, labels are difficult to acquire and much of machine learning is about to develop methods that allow to estimate or predict the labels of a data point based on its features.  \n",
    "\n",
    "2. A hypothesis space (also referred to as a ML model) consisting of computationally feasible predictor functions.\n",
    "\n",
    "3. A loss function that is used to assess the quality of a particular predictor function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d8e2f95e32775431d526103bfc1ba3b1",
     "grade": false,
     "grade_id": "cell-ccfc045530551950",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Learning Goals\n",
    "\n",
    "* Learn to make useful definitions for what data points (examples, samples), features and labels are in different real-life applications. \n",
    "* Learn how to represent data as numpy arrays which are, in turn, the Python implemenation of vectors and matrices.   \n",
    "* Learn to use (\"toy\") datasets provided by the Python library `scikit-learn`. \n",
    "* Learn about the concept of hypothesis spaces. \n",
    "* Learn how to fit (linear) predictions functions to data. \n",
    "\n",
    "This notebook contains several student tasks which require you to write a few lines of Python code to solve small problems. In particular, you have to fill in the gaps marked as **Student Task**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7f4d70080fb7f99ed8b2d2a07e288ec9",
     "grade": false,
     "grade_id": "cell-cce73a5c84faeb0f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<b><center><font size=4>Additional material</font></center></b>\n",
    "\n",
    "<b><font size=4>Videos</font></b>\n",
    "\n",
    "* [Data](https://youtu.be/WWYRH3x7_5M), [Hypothesis Space](https://youtu.be/CDcRfak1Mh4), [Hypothesis Space of Linear Models](https://youtu.be/Mch5hmhVuiA), [Hypothesis Space of Decision Trees](https://youtu.be/0FmaLfjAaRE), [Hypothesis Space of Deep Learning](https://youtu.be/im8mweIrpAM),[Loss Functions](https://www.youtube.com/watch?v=Uv9lihDfsBs&t=4s)\n",
    "\n",
    "<b><font size=4>Tutorials</font></b>\n",
    "\n",
    "* components of ML can be found under [this link](https://arxiv.org/pdf/1910.12387.pdf) \n",
    "\n",
    "* Python library `numpy` can be found under [this link](https://hackernoon.com/introduction-to-numpy-1-an-absolute-beginners-guide-to-machine-learning-and-data-science-5d87f13f0d51).\n",
    "\n",
    "* \"Learn the Basics\" and \"Data Science Tutorial\" sections from [this link](https://www.learnpython.org/en/).\n",
    "\n",
    "* a quick refresher for basic properties of matrices can be found under [this link](http://math.mit.edu/~gs/linearalgebra/linearalgebra5_1-3.pdf)\n",
    "and [this link](https://onlinelibrary.wiley.com/doi/pdf/10.1002/9780470549094.app1)\n",
    "\n",
    "* mathematical notation [this link](https://en.wikipedia.org/wiki/List_of_mathematical_symbols)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4dedad0e8e48117b4f2bfbe79b8e45db",
     "grade": false,
     "grade_id": "cell-7218f90089d3bf28",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Data as Matrices and Vectors\n",
    "<a id=\"Q1\"></a>\n",
    "\n",
    "To implement ML methods, we need to be able to efficiently **store and manipulate** data.  A quite powerful tool to represent and manipulate data are [vectors and matrices](https://en.wikipedia.org/wiki/Matrix_(mathematics)) which are, in turn, special cases of [tensors](https://en.wikipedia.org/wiki/Tensor). \n",
    "\n",
    "The data points arising in many application domains can often be characterized by a list of numeric attributes. This numeric attributes or \"features\", $x_{r}$ can be stacked conveniently into a vector $\\mathbf{x}=\\big(x_{1},\\ldots,x_{n}\\big)^{T}$. Many ML methods, such as linear regression (see round 2) or logistic regression (see round 3), use predictor functions of the form $h(\\mathbf{x}) = \\mathbf{w}^{T} \\mathbf{x}$ with some weight vector $\\mathbf{w}$. \n",
    "\n",
    "Once we restrict ourselves to linear functions of the form $h(\\mathbf{x}) = \\mathbf{w}^{T} \\mathbf{x}$, we can represent a predictor function by the weight vector $\\mathbf{w}$. Indeed, given the weight vector $\\mathbf{w}$, we can evaluate the predictor function for any feature vector $\\mathbf{x}$ as $h(\\mathbf{x}) = \\mathbf{w}^{T} \\mathbf{x}$. Thus, not noly we can represent data using a vector, but also the predictor functions applied to this data. \n",
    "\n",
    "Assume we have a set of data points which we index with $i=1,...,m$. The $i$th data point is characterized by the feature vector $\\mathbf{x}^{(i)} = \\big( x_{1}^{(i)}, \\ldots, x^{(i)}_{n} \\big)^{T}$ \n",
    "Accepted way to organize the data in ML is following: features are stored in the (\"feature\") matrix **X** with each row containing the data for each data point ($m$ - number of data points) and with each column storing the data of each feature vector ($n$ - number of features):\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{X}  = \\begin{pmatrix} X_{1,1} & X_{1,2}& \\ldots & X_{1,n} \\\\ \n",
    "X_{2,1} & X_{2,2}& \\ldots & X_{2,n} \\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\vdots \\\\ \n",
    "X_{m,1} & X_{m,2} & \\ldots & X_{m,n} \\end{pmatrix}\\in \\mathbb{R}^{m \\times n}   \\quad \\quad (Eq.1)\n",
    "\\end{equation} \n",
    "\\\n",
    "The matrix $\\mathbf{X} \\in \\mathbb{R}^{m \\times n}$ is stored in Python as a numpy array of shape (m,n). The $i$th row of the matrix $\\mathbf{X}$ is the feature vector $\\mathbf{x}^{(i)}$ of the $i$th data point. \n",
    "\n",
    "Labels of data points are stored in vector **y**: \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{y}  = \\begin{pmatrix} y_{1} \\\\ y_{2} \\\\ \\vdots \\\\ y_{m} \\end{pmatrix}\\in \\mathbb{R}^{m}\n",
    "\\end{equation} \n",
    "\\\n",
    "**y** vector is represented as a numpy array of shape (m,1)\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "$m$ - number of data points\\\n",
    "$n$ - number of features\\\n",
    "$\\mathbf{X}$       - upper-case bold letters denote a matrix  \\\n",
    "$\\mathbf{x}$       - lower-case bold letters denote a vector  \\\n",
    "$\\mathbf{x}^{T}$   - transpose of vector x \\\n",
    "$x_{1}$            - first entry of vector x\\\n",
    "$x_{r}$            - $r$th entry of vector x\\\n",
    "$\\mathbf{x}^{(i)}$ - feature vector of $i$th data point\\\n",
    "$x_{r}^{(i)}$      - $r$th feature of $i$th data point\\\n",
    "$\\mathbb{R}$       - real numbers\\\n",
    "$\\mathbb{R}^{n}$   - [real coordinate space](https://en.wikipedia.org/wiki/Real_coordinate_space) consisting of length-$n$ lists of real numbers \\\n",
    "$\\mathbb{R}^{m \\times n}$ - matrices with $m$ rows and $n$ columns of real-valued numbers$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features and Labels "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us illustrate the main ML terminology using a concrete example. Imagine that we want to build a model for classifying songs according to their genre (such as \"Pop\", \"Blues\" or \"Hip-Hop\"). In this case the **data points** will be songs, one particular song correspond to one particular data point. To build a classifier for the song genre, we need some labeled data points, i.e., songs for which we know the correct genre. Each data point has several   **features**, which characterize the songs. Features include e.g., the city where the song was produced, the length of the song's lyrics, its tempo or even the power spectrum of audio signal. The quantity of interest or **label** in this case is the genre to which the song belongs to. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../../coursedata/R1_ComponentsML/FeaturesLabels.jpg\" alt=\"Drawing\" style=\"width: 1000px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d39bc00c25a6bf24722237c1731252dc",
     "grade": false,
     "grade_id": "cell-5286954cef704fd2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='Bonus1'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <b>Bonus Task.</b> Machine Learning in your life. \n",
    "    \n",
    "Bonus task worth of 50 points.\n",
    "    \n",
    "Produces a short video/slides/description where some real-life situation is modelled as a machine learning problem. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f17f6ceca1b11bd1bc9f4756f75b275c",
     "grade": false,
     "grade_id": "cell-f22c842fc8d33ae5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Scikit-Learn Data\n",
    "\n",
    "The Python library `scikit-learn` comes with a few standard datasets, for instance the [iris](https://scikit-learn.org/stable/datasets/index.html#iris-plants-dataset) and [digits](https://scikit-learn.org/stable/datasets/index.html#optical-recognition-of-handwritten-digits-dataset) datasets for classification and the [boston house prices](https://scikit-learn.org/stable/datasets/index.html#boston-house-prices-dataset) and [linnerrud](https://scikit-learn.org/stable/datasets/index.html#linnerrud-dataset) datasets for regression.\n",
    "These are [Toy datasets](https://scikit-learn.org/stable/datasets/index.html#toy-datasets) - small datasets that do not require to download any file from some external websites. However, `sciki-learn` also provides significantly larger datasets that are referred to as [Real world datasets](https://scikit-learn.org/stable/datasets/index.html#real-world-datasets) which can be accessed online. \n",
    "\n",
    "Find more information about `scikit-learn` datasets here: https://scikit-learn.org/stable/datasets/index.html\n",
    "\n",
    "More datasets can be found here:\n",
    "https://archive.ics.uci.edu/ml/index.php\n",
    "https://www.kaggle.com/datasets\n",
    "\n",
    "Let us now take a closer look on some of these `scikit-learn` datasets and try to identify features and labels for these datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dd606ff289bb7817d7a55491233d7c18",
     "grade": false,
     "grade_id": "cell-33131fc4ff3f917e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Toy datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3590a8a508bcafadfe2c282e15fd362a",
     "grade": false,
     "grade_id": "cell-12d9b73f716ec849",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The code snippet below shows how to download datasets from `sklearn` and how to access features and labels of the data points in these datasets. Small toy datasets are imported using command `from sklearn import datasets`. \n",
    "\n",
    "These datasets are stored using the [`bunch` data type](https://pypi.org/project/bunch/), which is similar to the `dictionary` data type. A `bunch` object containes key-value pairs. Most datasets contain at least the keys `'data', 'target', 'target_names','DESCR'`. The value of the key `DESCR` is a short description of the dataset. The value of the `'target_names'` and `'target'` keys are the labels' names and labels, respectively, for each data point. \n",
    "By default, the labels of data points are always numbers. \n",
    "\n",
    "In a classification problem, these numbers are integers starting from $0$. The values of the key ``target_names`` provide a textual description of the meaning of different label values. E.g., the labels of images could be $y=0$ or $y=1$ and the label names would be 0=\"Cat\", 1=\"Dog\". \n",
    "The value of the `'data'`key is the feature matrix (see <a href='#Q1'>(Eq.1)</a>). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "160bc7d76742342ed12d06ecb82aa6ed",
     "grade": false,
     "grade_id": "cell-3c309834973bfbfe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<b><center><font size=3>Explore the dataset</font></center></b>\n",
    "The \n",
    "**[\"Digits\" dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits)** contains images of hand-written digits. This dataset can be used for testing a classification method to [recognize digits from hand-written images](https://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html#recognizing-hand-written-digits).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3144c25576c9cf1501007a2718a47adc",
     "grade": false,
     "grade_id": "cell-8ddb11cc7ec86ae8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# import toy datasets from sklearn library\n",
    "from sklearn import datasets \n",
    "\n",
    "# load the digits dataset into the bunch object \"digits\"\n",
    "digits = datasets.load_digits() \n",
    "# print the keys of all (key,value) pairs contained in digits\n",
    "digits.keys() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5cac1923cf3b1a7f94d8a8485e465d02",
     "grade": false,
     "grade_id": "cell-d1283347e1eeb490",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(digits.DESCR[:660]) # print out a short description of the dataset (only first 660 characters are used)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "47e5c43eb0d77c840a86a4071471ab7b",
     "grade": false,
     "grade_id": "cell-70ffe5a53ae5b399",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We can see from the description of the dataset that the **data points** are characterized by 8x8 pixel images of hand-written digits. Each pixel of an image is represented by an integer $0,\\ldots,16$ with $0$ meaning black pixel and $16$ meaning white pixel. \n",
    "\n",
    "Thus, each data point is characterized by 64 **features**, which are the integer values of the 64 pixels. Moreover, each data point is assigned a **label**, being an integer $0,\\ldots,9$, according to the digit which is shown in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print number of datapoints\n",
    "print(\"Number of datapoints: {}\".format(digits.data.shape[0]))\n",
    "\n",
    "# print the number of features used to characterize a single data point\n",
    "print(\"\\nNumber of features used to characterize a data point: {}\".format(digits.data.shape[1]))\n",
    "\n",
    "# print number of different classes (number of different values the label can take on) \n",
    "print(\"\\nNumber of different classes (different values the label can take on): {}\".format(digits.target_names))\n",
    "\n",
    "# print number of datapoints with known label\n",
    "print(\"\\nNumber of labeled data point: {}\".format(digits.target.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# digits.images returns an np.array of shape(m,8,8), which represent\n",
    "# m different images each having 8x8 pixels with grayscale value in the range 0..16.\n",
    "\n",
    "print(\"shape of digits.images : \",digits.images.shape)\n",
    "print(\"first image : \", digits.images[0,:,:])\n",
    "\n",
    "\n",
    "# digits.data reutrns an np.array of shape (m,6), which represent\n",
    "# m different images each represented by 64 numbers that are the \n",
    "# grayscale values of the image pixels \n",
    "\n",
    "print(\"shape of digits.data : \",digits.data.shape)\n",
    "print(\"data : \", digits.data)\n",
    "\n",
    "# note that the two different numpy arries digits.images and digits.data contain \n",
    "# exactly the same information but in a slightly different form \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt # import matplotlib library for plotting\n",
    "\n",
    "# plot the 8x8 images of the first 100 data points \n",
    "# each data point represents a handwritten digit\n",
    "\n",
    "fig, axes = plt.subplots(10, 10)    # create an array of subplots, 10 rows and 10 columns\n",
    "axes_flat = axes.flatten()          # collaps 10 x 10 array into a 1-dimensional array of 100 elements \n",
    "for i, ax in enumerate(axes_flat):  # iterate over array of subplots \n",
    "    ax.imshow(digits.images[i], cmap='gray_r')  # for i-th subplot, show the image of the i-th handwritten digit  \n",
    "    \n",
    "plt.setp(axes_flat, xticks=[], yticks=[], frame_on=False) # remove ticks and frame in all subplots\n",
    "plt.tight_layout(h_pad=0.5, w_pad=0.01)  # reduce whitspaces between plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "584dcd28b7b78711fd31f007531562e8",
     "grade": false,
     "grade_id": "cell-2c2a7ef97a6dd04c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<b><center><font size=3>Features vs Labels</font></center></b>\n",
    "\n",
    "The **[\"Linnerud\" dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_linnerud.html#sklearn.datasets.load_linnerud)** contains physiological parameters (Weight, Waist and Puls) and exercise data (Chins, Situps and Jumps) for 20 athletes. We can model this as a machine learning problem by considering data points representing athletes.\n",
    "\n",
    "In the description of the dataset the exercise data is referred to as features while the physiological parameters are considered the labels of datapoints (athletes). However, in practice one can choose what is the quantity of interest (labels). For example, if the quantity of interest is the number of jumps, one can use the physiological parameters (weight, waist and puls) as features to find a predictor for the number of jumps that the athelete is likely to achieve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the linnerud dataset into the bunch object \"linnerud\"\n",
    "linnerud = datasets.load_linnerud() \n",
    "\n",
    "# print features names\n",
    "print(\"\\nFeatures: {}\".format(linnerud.feature_names))\n",
    "\n",
    "# print labels names\n",
    "print(\"\\nLabels: {}\".format(linnerud.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7aef2a54a98c601a50db77069e48fa79",
     "grade": false,
     "grade_id": "cell-2de74c0d8ab9e319",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Real-World (Large) Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "87b0a6b1b4968bc429e01dafd194973b",
     "grade": false,
     "grade_id": "cell-9437de9563e1c37e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In addition to small datasets it is possible to fetch larger datasets using the library `scikit-learn`.\n",
    "\n",
    "**The Labeled Faces in the Wild face recognition** dataset is a collection of JPEG pictures of famous people collected over the internet (read more [here](https://scikit-learn.org/stable/datasets/index.html#the-labeled-faces-in-the-wild-face-recognition-dataset)). \n",
    "The dataset can be used to test methods for face verification or [face recognition](https://scikit-learn.org/stable/auto_examples/applications/plot_face_recognition.html#faces-recognition-example-using-eigenfaces-and-svms) classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_lfw_people # load data with sklearn.datasets \n",
    "\n",
    "# load the part of dataset with labeled images\n",
    "# use only persons with at least 20 images in the dataset, resize each picture by 0.4 ratio\n",
    "lfw_people = fetch_lfw_people(min_faces_per_person=20, resize=0.4) \n",
    "\n",
    "# print the keys of all (key,value) pairs in the lfw_people \n",
    "print(\"Dataset contain: {}\".format(lfw_people.keys()))\n",
    "\n",
    "# print number of datapoints\n",
    "print(\"\\nNumber of datapoints: {}\".format(lfw_people.data.shape[0]))\n",
    "\n",
    "# print the number of features used to charactize a data point\n",
    "print(\"\\nNumber of features: {}\".format(lfw_people.data.shape[1]))\n",
    "\n",
    "# print label or category names of the first 10 data points \n",
    "print(\"\\nLabels: {}\".format(lfw_people.target_names[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the image from the dataset\n",
    "plt.imshow(lfw_people.images[42], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f50c9ddb2cfa4c0c3e3f0ed0ba869bf6",
     "grade": false,
     "grade_id": "cell-9caa25de1ebab052",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Visualizing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b98334525a8233fe7868e995316086dc",
     "grade": false,
     "grade_id": "cell-e621d21ef7385ef9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "A very useful first step in applying ML methods is to visually inspect the dataset. Let us load and plot the [wine](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html#sklearn-datasets-load-wine) dataset from `sklearn`. We then visualize this data using some plotting functions provided by the Python libraries `pandas` ([`pd.plotting.scatter_matrix()` function](https://pandas.pydata.org/docs/reference/api/pandas.plotting.scatter_matrix.html)) and `seaborn` ([`sns.pairplot()` function](https://seaborn.pydata.org/generated/seaborn.pairplot.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "afac56cf41399101b281d6d6dd3cf44d",
     "grade": false,
     "grade_id": "cell-5bd0c981a64e942a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# `pandas` is a a fast and easy to use open source library for data analysis and manipulation \n",
    "import pandas as pd\n",
    "\n",
    "# Loading wine dataset from `sklearn` datasets\n",
    "wine = datasets.load_wine() # loading the wine dataset from sklearn datasets\n",
    "# print keys of all (key,value) pairs in the bunch oject \"wine\"\n",
    "print(wine.keys())\n",
    "\n",
    "X = wine['data']   # read in the value for the key \"data\"\n",
    "y = wine['target'] # read in the value for the key \"target\"\n",
    "\n",
    "# print out the shape of the numpy arrays X and y \n",
    "print('data:\\t\\t', X.shape, '\\nlabels shape:\\t', y.shape) # print number of elements along each dimension of \"X\" and \"Y\" \n",
    "print('------------------------------------------------') # print some dashes\n",
    "print(wine['feature_names']) # print the feature names in the dataset, key is a string, value is an array of string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "510b32bf29302e24f3ce275555661914",
     "grade": false,
     "grade_id": "cell-0588aff4d9ae2688",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The code snippet below generates scatterplots using different combinations of features for data points in the wine dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4f58c82d45719d43bf84f63de7fc144c",
     "grade": false,
     "grade_id": "cell-c3201b3787204aa6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# transform np.array X to pandas dataframe, choose only first 3 features\n",
    "wine_dataframe =  pd.DataFrame(X[:,:3], columns=wine['feature_names'][:3])\n",
    "\n",
    "# plot pandas dataframe with histogram and scatter plots\n",
    "pd.plotting.scatter_matrix(wine_dataframe, c=y, figsize=(8,8), s=120)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b97e1652cd7494856e564d1ba79da2fa",
     "grade": false,
     "grade_id": "cell-026c5a0e8f9e5f57",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns # import seaborn library for plotting\n",
    "\n",
    "# transform np.array X to pandas dataframe, choose only first 3 features\n",
    "wine_dataframe =  pd.DataFrame(X[:,:3], columns=wine['feature_names'][:3])\n",
    "wine_dataframe['wine category'] = y\n",
    "\n",
    "# plot pandas dataframe\n",
    "# the lines shows the density plot which is essentially a smooth version of the histogram\n",
    "sns.pairplot(wine_dataframe, hue = 'wine category', vars=wine['feature_names'][:3])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "35425936feb61f1e273d79847e3c38fd",
     "grade": false,
     "grade_id": "cell-bd82f3e1536b9a6d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<a id=\"Q5\"></a>\n",
    "\n",
    "## Learning from Data by Fitting a Model (Curve)\n",
    "\n",
    "The `Linnerud dataset` consists of data points that represent athletes doing some exercise. We might be interested in how specific properties (or features) of a person affect their performance. Let us try to predict the number of chin-ups that an athlete can do.  \n",
    "\n",
    "We can formalize this as a machine learning problem. The data points represent athletes who are characterized by the feature $x$ given by the body weight in kg. The quantity of interest (the label) $y$ of a data point is the number of chin-ups the athlete can do. Our goal is to find a predictor function $h(x)$ which takes the bodyweight $x$ as input and outputs a predicted label $\\hat{y}$ which estimates the number of chin-ups that the athlete should be able to do. \n",
    "\n",
    "Choosing a good predictor $h(x)$ from the space of all possible functions $h(\\cdot): \\mathbb{R} \\rightarrow \\mathbb{R}$ is challenging since there are [**so many** of these functions](https://en.wikipedia.org/wiki/Function_of_a_real_variable#Cardinality_of_sets_of_functions_of_a_real_variable). Therefore, we restrict ourselves to the space of linear functions\n",
    "\\begin{equation}\n",
    "h^{(w)}(x) = w \\cdot x. \n",
    "\\end{equation} \n",
    "The set of all of such functions, obtained for different choices for $w$, constitutes the hypothesis space of linear predictors. \n",
    "Each function of this **hypothesis space** is characterized by a single number $w \\in \\mathbb{R}$. Once we specify this number (or weight), we can compute the function value $h^{(w)}(x)$ for any possible feature value $x$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "68cf104d276d24fca7fc7c13ecb4d210",
     "grade": false,
     "grade_id": "cell-17cd17a07c82795a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<img src=\"../../../coursedata/R1_ComponentsML/Hspace.jpg\" alt=\"Drawing\" style=\"width: 1000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "82a72cca722bbc4fb6c240db9905795d",
     "grade": false,
     "grade_id": "cell-db073f7fcf8a47d7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='Bonus2'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <b>Bonus Task.</b> Hypothesis space. \n",
    "    \n",
    "Bonus task worth of 50 points.\n",
    "    \n",
    "Prepare a short video that explains the concept of a hypothesis space. The video must not be longer than 10 minutes. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4f73c535452f7b9796b809168f1c3259",
     "grade": false,
     "grade_id": "cell-682234cedb587efc",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<a id='HypothesisDemo'></a>\n",
    "<div class=\" alert alert-info\">\n",
    "    <b>Demo.</b> Hypothesis Space of Linear Predictor Functions.  \n",
    "    \n",
    "The code snippet below creates a scatterplot of the `Linnerud` dataset and also plots \n",
    "some of the predictor functions from the linear hypothesis space. These predictor functions \n",
    "are of the form $h(x) = w \\cdot x$ with given weight $w$.\n",
    "\n",
    "Hint: In this section, we will use the Python library [Scikit-learn (Sklearn)](https://scikit-learn.org/stable/index.html) to fit models to data, specifically we will use [linear regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn-linear-model-linearregression)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7d5828e73ea5e42aec482fd17df1bb31",
     "grade": false,
     "grade_id": "cell-6fadac9f2f063914",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# the library \"numpy\" provides functions for matrices and vectors\n",
    "import numpy as np\n",
    "# import matplotlib library for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "# import toy datasets from sklearn library\n",
    "from sklearn import datasets \n",
    "# import module for linear regression \n",
    "from sklearn import linear_model \n",
    "\n",
    "# load Linnerud dataset into bunch object `linnerud`\n",
    "linnerud = datasets.load_linnerud()  \n",
    "# read out the values for the key 'data' and store them in the numpy array ChinUps \n",
    "ChinUps = linnerud['data']  \n",
    "# read out the values for the key 'target' and store them in the numpy array Weight\n",
    "Weight = linnerud['target']  \n",
    "\n",
    "# we use the weight (in Lbs) of each athlete as features \n",
    "x = Weight.T[0] \n",
    "# we use the number of chin ups performed by each athlete as label \n",
    "y = ChinUps.T[0] \n",
    "x = x.reshape(-1,1)  # convert to numpy array of shape (m,1)\n",
    "y = y.reshape(-1,1)  # convert to numpy array of shape (m,1)\n",
    "x = x*0.453 # convert Lbs to Kg\n",
    "\n",
    "# initialize the linear regression model\n",
    "reg = linear_model.LinearRegression(fit_intercept=False)\n",
    "# fit the linear regression model with variables \"x\" and \"y\"\n",
    "# to create the reg.coef_  (weight) attribute \n",
    "reg.fit(x,y)\n",
    "\n",
    "# initialize and empty list\n",
    "hypothesis_space = [] \n",
    "# generate 10 linear predictors\n",
    "for i in range(10): # loop over range 0-10\n",
    "    reg.coef_ = np.array([[i*0.05]]) # set new regression coefficient (weight)\n",
    "    n = reg.predict(x) # make predictions based on the previously defined and fitted regression model\n",
    "                       # but with new regression coefficient (weight)\n",
    "    hypothesis_space.append(n) # append the preditions to \"hypothesis_space\" list\n",
    "\n",
    "# plot the datapoints and generated predictor functions from the linear hypothesis space\n",
    "# initialize subplots and get \"fig\" and \"axes\" objects\n",
    "fig, axes = plt.subplots(1, 1, figsize=(12, 6))  \n",
    "\n",
    "# initialize a scatterplot\n",
    "axes.scatter(x, y, color='blue',label=\"data points\") \n",
    "# plot items from the \"hypothesis_space\" list\n",
    "for i in range(len(hypothesis_space)): # loop through the items in \"hypothesis_space\" list\n",
    "    y_n = hypothesis_space[i] # plot the ith item from the \"hypothesis_space\" list\n",
    "    l = 'w = {:.2f}'.format((i)*0.05) # get a formatted string to use in legend\n",
    "    axes.plot(x, y_n, label=l) # add the item from \"hypothesis_space\" to the plot\n",
    "\n",
    "plt.rc('legend', fontsize=10) # update plot fonts\n",
    "plt.rc('axes', labelsize=20)  # update plot fonts\n",
    "plt.rc('xtick', labelsize=20) # update plot fonts\n",
    "plt.rc('ytick', labelsize=20) # update plot fonts\n",
    "plt.rc('font', size=20)       # update plot fonts\n",
    "\n",
    "axes.set_title('Several different linear predictor functions') # set plot title\n",
    "axes.set_xlabel('body weight (kg)') # set x-axis label\n",
    "axes.set_ylabel('number of chinups') # set y-axis label\n",
    "axes.legend(loc='upper left') # set location of the legend to show in upper left corner\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "96761712f1206bee47e44ec297015121",
     "grade": false,
     "grade_id": "cell-1ed770e7af6187b0",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "\n",
    "### Learning the Best Predictor by Fitting a Model\n",
    "\n",
    "A key idea underlying ML is to choose predictor functions $h(x)$ based on how well they fit historic or training data. For linear predictors, of the form $h(x)=w \\cdot x$, the search of a good predictor reduces to the search for a good choice for the weight $w \\in \\mathbb{R}$ which is just a number. \n",
    "\n",
    "To search for a good predictor function $h$, which maps a feature value $x$ to the predicted label $\\hat{y}=h(x)$, we need to measure the loss (or error) incurred when the true label is $y$ but the predicted label is $\\hat{y}$. There are many different choices for how to define such a loss function. \n",
    "\n",
    "In general, we are free to define the loss function to best suit the application at hand. However, for certain classes of machine learning problems some useful choices for the loss functions have crystalized. For example, if the labels of data points take on numeric values, a widely used choice for the loss function is the squared error loss $(y - \\hat{y})^{2}$. \n",
    "\n",
    "Using loss functions to measure the quality of a predictor requires the availability of data points for which we know the true label $y$. One option to get labeled data is from historic recordings or experiments. Assume we have some labeled data points $(x^{(1)},y^{(1)}),\\ldots,(x^{(m)},y^{(m)})$ consisting of $m$ data points. The $i$th data point has the feature $x^{(i)}$ and the true label $y^{(i)}$. \n",
    "\n",
    "We have now all the tools to find the best linear predictor $h(x) = w \\cdot x$ by minimizing the empirical risk or average squared error loss \n",
    "\\begin{equation}\n",
    "(1/m) \\sum_{i=1}^{m} (y^{(i)} - \\hat{y}^{(i)})^{2} = (1/m) \\sum_{i=1}^{m} (y^{(i)} - w \\cdot x^{(i)})^{2}. \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One might wonder if we could use the prediction error $y - \\hat{y}$  itself as a loss function. It can be easily shown that the prediction error (without squaring) is not a good choice as loss function. Indeed, ML methods try find a predictor which minimizes the (average) loss incurred over some labeled data points (training data). Using the prediction error as loss, this minimization problem would result in a trivial predictor $\\hat{y}$ which outputs a number as large as possible (limited by the used number format). Indeed, making $\\hat{y}$ as large as possible would \n",
    "make $y-\\hat{y}$ as small as possible. The resulting predictor would completely ignore the training data and therefore will not be useful at all. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0aa33673c574e274f8a855f517a6a982",
     "grade": false,
     "grade_id": "cell-cb8925a42313f816",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<a id='FitRegressionDemo'></a>\n",
    "<div class=\" alert alert-info\">\n",
    "    <b>Demo.</b> Fitting a Linear Model to Data.  \n",
    "\n",
    "A linear model corresponds to the set of linear predictor $h(x) = w \\cdot x$ with some weight $w\\in \\mathbb{R}$. Fitting a linear model means to choose the weight to minimize the average prediction error $y-h(x)$ incurred for some labeled data points. The optimal weight $w_{\\rm opt}$ can be computed via the function `LinearRegression.fit()` and the corresponding prediction $\\hat{y} = w_{\\rm opt} x$ for a data point with feature $x$ can be computed using `LinearRegression.predict()`. \n",
    "\n",
    "We will plot the data points along with the predictions $\\hat{y}^{(i)} = w_{\\rm opt} x^{(i)}$ and the prediction errors $y^{(i)} - \\hat{y}^{(i)} = y^{(i)} - w_{\\rm opt} x^{(i)}$ as red bars. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3140eb043c9d0a59f04e65c0b96d5ed0",
     "grade": false,
     "grade_id": "cell-1ad68ef75f613b5c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# plot datapoints, linear predictor with optimal weight and prediction errors  \n",
    "# we will use values of variables `x` (features, weight) and `y` (labels, chin ups) \n",
    "# generated in the previous cell \n",
    "\n",
    "plt.rc('font', size=20) # set plot font\n",
    "\n",
    "reg = linear_model.LinearRegression(fit_intercept=False) # initialize Linear Regression model\n",
    "reg.fit(x, y) # fit the linear regression model using \"x\" and \"y\" variables\n",
    "y_pred = reg.predict(x) # make predictions based on fitted model\n",
    "\n",
    "# print weight of the optimal predictor \n",
    "print(\"optimal weight w =\", reg.coef_[0][0])\n",
    "\n",
    "# initialize subplots and get \"fig\" and \"axes\" objects\n",
    "fig, axes = plt.subplots(1, 1, figsize=(8, 4))\n",
    "# initialize a scatterplot with horizontal (vertical) axis representing feature (label) values \n",
    "axes.scatter(x, y, label='data points') \n",
    "# add the predicted labels \"y_pred = h(x)\" made by the model to the plot\n",
    "axes.plot(x, y_pred, color='green', label='optimal linear predictor') \n",
    "\n",
    "# indicate error bars\n",
    "\n",
    "axes.plot((x[0], x[0]), (y[0], y_pred[0]), color='red', label='errors') # add label to legend\n",
    "for i in range(len(x)-1): # loop through range length of x - 1\n",
    "    lineXdata = (x[i+1], x[i+1]) # make tuples with same X\n",
    "    lineYdata = (y[i+1], y_pred[i+1]) # make tuples with different y's\n",
    "    axes.plot(lineXdata, lineYdata, color='red') # add the red lines to the plot to indicate error distance from our predicted regression model\n",
    "\n",
    "# add legend to the plot and set position\n",
    "axes.legend(loc='upper center', bbox_to_anchor=(1.4, 1.05),fontsize=20) \n",
    "# set axes labels\n",
    "axes.set_xlabel(\"feature x (body weight)\")\n",
    "axes.set_ylabel(\"label y (number of chin-ups)\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3e71b26b8e707c2d02b6a3ce390ea3fd",
     "grade": false,
     "grade_id": "cell-16037415de0db9f3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Adding an Intercept Term \n",
    "\n",
    "A simple but useful extension of the linear prediction functions used above is to add an intercept term. In particular, we consider predictor functions of the form \n",
    "\\begin{equation}\n",
    "h(x) = w \\cdot x + b \n",
    "\\end{equation}\n",
    "which involves a weight $w$ and a constant offset $b$. The offset $b$ is sometimes referred to as the \"intercept term\" and geometrically it is value at which the regression line crosses the y-axis. The code snippet below finds (or learns) the best choices for the weight $w$ and intercept $b$ in order to minimize the average squared error incurred for a given set of labeled data points $(x^{(i)},y^{(i)})$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "38536662550b1d5300a59aea24c8a45c",
     "grade": false,
     "grade_id": "cell-765366fec7df14e4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# build linear models without (fit_intercept=False) and with intercept (fit_intercept=True)\n",
    "\n",
    "# initialize Linear Regression model without intercept (fit_intercept=False) \n",
    "reg = linear_model.LinearRegression(fit_intercept=False) \n",
    "reg.fit(x, y) # fit the linear regression model using \"x\" and \"y\" variables\n",
    "y_pred = reg.predict(x) # make predictions based on fitted model\n",
    "# find the optimal weight for linear regression model (without intercept)\n",
    "w_opt = reg.coef_[0][0]\n",
    "\n",
    "# initialize Linear Regression model with intercept (fit_intercept=True)\n",
    "reg_intercept = linear_model.LinearRegression(fit_intercept=True) \n",
    "reg_intercept.fit(x, y) # fit the linear regression model using \"x\" and \"y\" variables\n",
    "y_pred_intercept = reg_intercept.predict(x) # make predictions based on fitted model\n",
    "# find the optimal weights for linear regression model (with intercept)\n",
    "w_opt_intercept = reg_intercept.coef_[0][0]\n",
    "# find the intercept for linear regression model (with intercept)\n",
    "intercept = reg_intercept.intercept_[0]\n",
    "\n",
    "# print parameters of the optimal predictors\n",
    "print(\"model without intercept: optimal weight w = {}\".format(w_opt))\n",
    "print(\"model wit intercept: optimal weight w = {} and intercept = {}\".format(w_opt_intercept,intercept))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ed04c708c23276285c525761f83b65d6",
     "grade": false,
     "grade_id": "cell-15abbfeb454adb97",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Note that the predictor function obtained without intercept term passes through origin, while the predictor function obtained with the intercept term crosses y-axis at the value of the intercept term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fa37ca2aa95fc737b8e270673e23659f",
     "grade": false,
     "grade_id": "cell-61b30db798a56dce",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# plot regression dataset\n",
    "plt.rc('font', size=20) # set plot font\n",
    "\n",
    "# create a grid of test feature values  \n",
    "x_grid = np.linspace(-10, 120, num=100).reshape(-1,1) \n",
    "# compute predictions from linear regression model without intercept term \n",
    "y_pred = reg.predict(x_grid) \n",
    "# compute predictions on test feature values using linear regression model with intercept term \n",
    "y_pred_intercept = reg_intercept.predict(x_grid)\n",
    "\n",
    "fig, axes = plt.subplots(1, 1, figsize=(8, 4))\n",
    "# initialize a scatterplot \n",
    "axes.scatter(x, y, label='data points') \n",
    "# add the predicted labels \"y_pred = h(x)\" made by the model to the plot\n",
    "axes.plot(x_grid, y_pred, color='green', label='no intercept') \n",
    "# add the predicted labels \"y_pred_intercept = h(x)\" made by the model to the plot\n",
    "axes.plot(x_grid, y_pred_intercept, color='red', label='intercept')\n",
    "\n",
    "# set axes labels\n",
    "axes.set_xlabel(\"feature x\") \n",
    "axes.set_ylabel(\"label y\")\n",
    "\n",
    "# add legend to the plot and set position\n",
    "axes.legend(loc='upper center', bbox_to_anchor=(1.25, 1),fontsize=20) # add a legend to the plot\n",
    "\n",
    "axes.axvline(0,c='k',ls=':') # add dotted line at x=0 to the plot\n",
    "axes.axhline(0,c='k',ls=':') # add dotted line at y=0 to the plot\n",
    "axes.axhline(intercept,c='k',ls=':') # add dotted line at y=intercept to the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2d58cefcc2b648ed46ad3a3d4286c38e",
     "grade": false,
     "grade_id": "cell-0ff92b94c941f4be",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Note that the optimal weight for the linear predictor without intercept is positive, which suggests a positive correlation between feature value and label value (higher feature value hints at higher label value). In contract, the optimal weight obtained for the linear predictor with an intercept term is negative. This negative weight would suggest a negative correlation between feature value and label value (higher feature value hints at lower label value). \n",
    "\n",
    "Since the training error obtained from the linear predictor with intercet term is smaller, it is more plausible to have a negative correlation between feature value and label value. This agrees with the intuition that having a higher body weight typically implies a smaller number of achievable chin-ups. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "81c9cd0dbe86680040851d5088f91f56",
     "grade": false,
     "grade_id": "cell-11aeade9e7ce5633",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='QuestionR1_1'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <b>Student Task.</b> Fitting a Linear Model without and with Intercept. \n",
    "\n",
    "Fit a linear regression model to a set of data points $(x^{(1)},y^{(1)}),\\ldots,(x^{(m)},y^{(m)})$ which are generated synthetically (using a random generator) and stored in the vectors `syn_x` and `syn_y`. Use the Python class `linear_model.LinearRegression` to represent the set of linear predictor functions. Find optimal weight for linear predictor without intercept `w_opt` and optimal weight and intercept `w_opt_intercept` and `intercept` for linear predictor with intercept. Plot regression dataset.\n",
    "\n",
    "Use `LinearRegression(fit_intercept=True)` when initializing the Linear regression model with intercept and `LinearRegression(fit_intercept=False)` when iinitializing it without and intercept.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6f7d83a7c4ae8e9732dd7831ec81cabc",
     "grade": false,
     "grade_id": "cell-e7ae566693783d23",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model # import \"linear_model\" from sklearn\n",
    "from sklearn.datasets import make_regression # import \"make_regression from sklearn.datasets\"\n",
    "\n",
    "# generate data using the imported \"make_regression\" function\n",
    "syn_x, syn_y = make_regression(n_samples=30, n_features=1, noise=30, random_state=1) \n",
    "syn_x = syn_x + 10*np.ones(30).reshape(syn_x.shape) # add a 10* identity matrix to data\n",
    "\n",
    "# plot regression dataset\n",
    "plt.rc('font', size=20) # change plot font\n",
    "\n",
    "### STUDENT TASK ###\n",
    "# create Linear Regression model without an intercept\n",
    "# reg = ...\n",
    "# fit a linear regression model (without intercept)\n",
    "# reg = ... \n",
    "# find the optimal weight for linear regression model (without intercept)\n",
    "# w_opt = ...\n",
    "\n",
    "# create Linear Regression model using an intercept term \n",
    "# reg_intercept = ...\n",
    "# fit a linear regression model (with intercept)\n",
    "# reg_intercept = ... \n",
    "# find the optimal weights for linear regression model (with intercept)\n",
    "# w_opt_intercept = ...\n",
    "# find the intercept for linear regression model (with intercept)\n",
    "# intercept = ...\n",
    "\n",
    "# remove the line raise NotImplementedError() before testing your solution and submitting code\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# print parameters of the optimal predictor \n",
    "print(\"model without intercept: optimal weight w = {}\".format(w_opt))\n",
    "print(\"model wit intercept: optimal weight w = {} and intercept = {}\".format(\n",
    "                                w_opt_intercept,intercept))\n",
    "\n",
    "# create a grid of test feature values  \n",
    "x_grid = np.linspace(-1, 16, num=100).reshape(-1,1) \n",
    "# compute predictions from linear regression model without intercept term \n",
    "y_pred = reg.predict(x_grid) \n",
    "# compute predictions on test feature values using linear regression model with intercept term \n",
    "y_pred_intercept = reg_intercept.predict(x_grid)\n",
    "\n",
    "fig, axes = plt.subplots(1, 1, figsize=(8, 4)) # initialize subplots and get \"fig\" and \"axes\" variables\n",
    "axes.scatter(syn_x, syn_y, label='data points') # create a scatter plot with the generated synthetic data\n",
    "axes.plot(x_grid, y_pred, color='green', label='no intercept') # add a line to the plot\n",
    "axes.plot(x_grid, y_pred_intercept, color='red', label='with intercept') # # add a line to the plot\n",
    "\n",
    "axes.legend() # add a legend to the plot\n",
    "axes.set_xlabel(\"feature x\") # add x-axis label to the plot \n",
    "axes.set_ylabel(\"label y\") # add y-axis label to the plot\n",
    "axes.axhline(y=0, color='k',linestyle=':') # add a dotted lien to the plot\n",
    "axes.axvline(x=0, color='k',linestyle=':') # add a dotted line to the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d18b136c0ff9ededb750bf78d915dba3",
     "grade": true,
     "grade_id": "cell-f9495c305f7fa466",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# this cell constains visible tests (sanity checks) and \n",
    "# hidden tests which are used for grading student solutions \n",
    "\n",
    "assert w_opt < 1, \"w_opt value is wrong\"\n",
    "assert w_opt_intercept < 50, \"w_opt_intercept value is wrong\"\n",
    "assert intercept > -400, \"intercept value is wrong\"\n",
    "\n",
    "\n",
    "print('Sanity check tests passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e55e5729e2e1f1b9a4b85d0388d8d572",
     "grade": false,
     "grade_id": "cell-07143459df52a5c0",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Take Home Quiz "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0a3bb9053c3a9b72b7282d5619152aa8",
     "grade": false,
     "grade_id": "cell-81144558954a5835",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Answer the following questions by setting the `answer_R1_Q??` variable for each question to the number of the correct answer. For example, if you think that the second answer in the first quiz question is the right one, then set `answer_R1_Q1=2`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ddc3afd2fd00e7a59d4bc7bb8d03a506",
     "grade": false,
     "grade_id": "cell-1aa1ddb20cf88962",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<a id='QuestionR1_1'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <b>Student Task.</b> Question R1.1. \n",
    "\n",
    "<p> Modify the code in the demo \"Fitting a Linear Model to Data\" to determine the difference (error) between the predicted value y_pred and the true label y for the first data point (which corresponds to the index 0). Select the correct value (rounded to one decimal) of this error below.</p>\n",
    "\n",
    "<ol>\n",
    "  <li>4.6</li>\n",
    "  <li>-3.8</li>\n",
    "  <li>5.0</li>\n",
    "  <li>7.5</li>\n",
    "</ol> \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2514fe10123725f50f059bc2b31ee4b2",
     "grade": false,
     "grade_id": "cell-be2c274c69a172dc",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# answer_Q1\n",
    "\n",
    "# remove the line raise NotImplementedError() before testing your solution and submitting code\n",
    "\n",
    "# answer_R1_Q1  = ...\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6e200b29d86603858ff5b08cd813e331",
     "grade": true,
     "grade_id": "cell-a37329cc76fcdc18",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# this cell is for tests\n",
    "\n",
    "assert answer_R1_Q1 in [1,2,3,4], '\"answer_R1_Q1\" Value should be an integer between 1 and 4.'\n",
    "print('Sanity check tests passed!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4594f7699bd395bfe01b8281a5858964",
     "grade": false,
     "grade_id": "cell-3046aeea89e6dec3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<a id='QuestionR1_2'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <b>Student Task.</b> Question R1.2. \n",
    "\n",
    "<p>Consider data points having features $\\mathbf{x}$ and a numeric label $y$. How can the quality of a predictor function $h(\\mathbf{x})$, which delivers a predicted label $\\hat{y}=h(\\mathbf{x})$ be measured to choose the best predictor function?</p>\n",
    "\n",
    "\n",
    "<ol>\n",
    "  <li>Use the difference between our predicted value $\\hat{y}$ and true label $y$, i.e: $y - \\hat{y}$ and pick the predictor function with the lowest value of loss.</li>\n",
    "  <li>Use the squared error loss $(y - \\hat{y} )^{2}$ and pick the predictor function with the highest value of squared error loss.</li>\n",
    "  <li>Use the squared error loss $(x - \\hat{x})^{2}$ and pick the predictor function with the lowest value of squared error loss.</li>\n",
    "  <li>Use the squared error loss $(y - \\hat{y})^{2}$ and pick the predictor function with the lowest value of the squared error loss.</li>\n",
    "</ol> \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c2b15f2a3be7989d265d93621ee1b01a",
     "grade": false,
     "grade_id": "cell-8b0c045a8dd164b0",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# answer_Q2\n",
    "\n",
    "\n",
    "\n",
    "# remove the line raise NotImplementedError() before testing your solution and submitting code\n",
    "answer_R1_Q2  = 4\n",
    "\n",
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "25898bfdd32855a647ccc4395f874d56",
     "grade": true,
     "grade_id": "cell-f8cf4a0b2c7d28bd",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# this cell is for tests\n",
    "\n",
    "assert answer_R1_Q2 in [1,2,3,4], '\"answer_R1_Q2\" Value should be an integer between 1 and 4.'\n",
    "print('Sanity check tests passed!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e644c05221b64bddf41f83fabc59064e",
     "grade": false,
     "grade_id": "cell-790a88a704d347ee",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='QuestionR1_3'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <b>Student Task.</b> Question R1.3. \n",
    "\n",
    "<p> Consider a set of $m=3$ data points represented by the feature vectors $\\mathbf{x}^{(1)}=\\big(1,0,0,0\\big)^{T}$, $\\mathbf{x}^{(2)}=\\big(1,0,1,0\\big)^{T}$ and $\\mathbf{x}^{(3)}=\\big(1,0,-1,0\\big)^{T}$. What is $x^{(2)}_{3}$ ? </p>\n",
    "\n",
    "<ol>\n",
    "  <li>1 </li>\n",
    "  <li>0</li>\n",
    "  <li>-1</li>\n",
    "</ol> \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4008f2d2b672d10738d32f71fa61622a",
     "grade": false,
     "grade_id": "cell-91248121300e8eb8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# answer_Q3\n",
    "\n",
    "# answer_R1_Q3  = ...\n",
    "# remove the line raise NotImplementedError() before testing your solution and submitting code\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7b585f7e31546d9e7147c81002eedf28",
     "grade": true,
     "grade_id": "cell-73b6333b16763e07",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# this cell is for tests\n",
    "\n",
    "assert answer_R1_Q3 in [1,2,3], '\"answer_R1_Q3\" Value should be an integer between 1 and 3.'\n",
    "print('Sanity check tests passed!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "304.8px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
